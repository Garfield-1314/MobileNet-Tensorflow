# Jupyter Notebook - ä»£ç 

# å¯¼å…¥å¿…è¦çš„åº“
import matplotlib.pyplot as plt
import numpy as np
import os
import tensorflow as tf
import seaborn as sns
from tqdm import tqdm
from sklearn.metrics import confusion_matrix
import tensorflow_model_optimization as tfmot  # æ–°å¢å‰ªæåº“
import datetime
# è®¾å®šæ—¥å¿—çº§åˆ«
tf.get_logger().setLevel('ERROR')

# ğŸ”¹ è¶…å‚æ•°
IMG_SIZE = (128, 128)
AUTOTUNE = tf.data.AUTOTUNE
# -------------------------------- ç¬¬ä¸€é˜¶æ®µè®­ç»ƒ ---------------------------------
# ğŸ”¹ æ•°æ®é›†è·¯å¾„
base_dir = './dataset'
train_dir = os.path.join(base_dir, '80')
valid_dir = os.path.join(base_dir, '80')

BATCH_SIZE = 128
# ğŸ”¹ åŠ è½½æ•°æ®é›†
train_dataset_raw = tf.keras.preprocessing.image_dataset_from_directory(
    train_dir, validation_split=0.2, subset="training", seed=12,
    batch_size=BATCH_SIZE, image_size=IMG_SIZE)

validation_dataset_raw = tf.keras.preprocessing.image_dataset_from_directory(
    valid_dir, validation_split=0.2, subset="validation", seed=12,
    batch_size=BATCH_SIZE, image_size=IMG_SIZE)

class_names = train_dataset_raw.class_names
print("Class Names:", class_names)

# é¢„å¤„ç†å‡½æ•°
def preprocess_image(image, label):
    return image, label

# åŠ è½½æ•°æ®é›†
train_dataset = (train_dataset_raw
                 .map(preprocess_image, num_parallel_calls=AUTOTUNE)
                 .cache()
                 .shuffle(1000)
                 .prefetch(AUTOTUNE))

validation_dataset = (validation_dataset_raw
                      .map(preprocess_image, num_parallel_calls=AUTOTUNE)
                      .cache()
                      .prefetch(AUTOTUNE))

# ğŸ”¹ æ„å»ºæ¨¡å‹
IMG_SHAPE = IMG_SIZE + (3,)
base_model = tf.keras.applications.MobileNetV2(
    input_shape=IMG_SHAPE, include_top=False, pooling = 'avg', alpha=0.35, weights='imagenet')

# å†»ç»“é™¤æœ€å4å±‚å¤–çš„æ‰€æœ‰å±‚
base_model.trainable = True
# for layer in base_model.layers[:-0]:
#     layer.trainable = False

model = tf.keras.Sequential([
    tf.keras.layers.Rescaling(1./255),
    base_model,
    # tf.keras.layers.GlobalAveragePooling2D(),
    # tf.keras.layers.Dropout(0.1),
    tf.keras.layers.Dense(len(class_names), activation='softmax')
])
model.build((None, 128, 128, 3))
model.summary()
# ç¼–è¯‘æ¨¡å‹
lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate=0.00001, decay_steps=1000, decay_rate=0.90, staircase=True)

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
              metrics=['accuracy'])

# è®­ç»ƒç¬¬ä¸€é˜¶æ®µ
early_stopping = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)

history_stage1 = model.fit(train_dataset,
                    validation_data=validation_dataset,
                    epochs=100, 
                    callbacks=[early_stopping]
                    )

# ä¿å­˜ç¬¬ä¸€é˜¶æ®µæ¨¡å‹
# model.save('./model/stage1_model.h5')
     
# -------------------------------- ç¬¬äºŒé˜¶æ®µè®­ç»ƒ ---------------------------------
# ğŸ”¹ æ•°æ®é›†è·¯å¾„
base_dir = './dataset'
train_dir = os.path.join(base_dir, '80')
valid_dir = os.path.join(base_dir, '80')

BATCH_SIZE = 128

# åŠ è½½æ•°æ®é›†
train_dataset_raw = tf.keras.preprocessing.image_dataset_from_directory(
    train_dir, validation_split=0.2, subset="training", seed=12,
    batch_size=BATCH_SIZE, image_size=IMG_SIZE)

validation_dataset_raw = tf.keras.preprocessing.image_dataset_from_directory(
    valid_dir, validation_split=0.2, subset="validation", seed=12,
    batch_size=BATCH_SIZE, image_size=IMG_SIZE)

# æ•°æ®é¢„å¤„ç†ï¼ˆåŒç¬¬ä¸€é˜¶æ®µï¼‰
train_dataset = (train_dataset_raw
                 .map(preprocess_image, num_parallel_calls=AUTOTUNE)
                 .cache()
                 .shuffle(1000)
                 .prefetch(AUTOTUNE))

validation_dataset = (validation_dataset_raw
                      .map(preprocess_image, num_parallel_calls=AUTOTUNE)
                      .cache()
                      .prefetch(AUTOTUNE))

early_stopping = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)
# åŠ è½½ç¬¬ä¸€é˜¶æ®µæ¨¡å‹
# model = tf.keras.models.load_model('./model/stage1_model.h5')
# ç¼–è¯‘æ¨¡å‹
lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate=0.00001, decay_steps=1000, decay_rate=0.90, staircase=True)

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
              metrics=['accuracy'])

# è®­ç»ƒç¬¬ä¸€é˜¶æ®µ
early_stopping = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)

history_stage2 = model.fit(train_dataset, validation_data=validation_dataset,
                    epochs=100, callbacks=[early_stopping])
# ä¿å­˜ç¬¬ä¸€é˜¶æ®µæ¨¡å‹
# model.save('./model/stage2_model.h5')
# -------------------------------- ç¬¬ä¸‰é˜¶æ®µè®­ç»ƒ ---------------------------------
# ğŸ”¹ å‰ªæå‚æ•°é…ç½®
PRUNING_PARAMS = {
    'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(
        initial_sparsity=0.30,
        final_sparsity=0.60,
        begin_step=0,
        end_step=2000,
        frequency=100
    )
}

# åŠ è½½ç¬¬äºŒé˜¶æ®µæ¨¡å‹
# model = tf.keras.models.load_model('./model/stage2_model.h5')

# ğŸ”¹ åˆ†ç¦» Rescaling å±‚å’ŒåŸºç¡€æ¨¡å‹
rescale_layer = model.layers[0]  # æå– Rescaling å±‚    
prunable_model = tf.keras.Sequential(model.layers[1:])  # æ’é™¤ Rescaling åçš„æ¨¡å‹

# ğŸ”¹ åº”ç”¨å‰ªæåˆ°å¯å‰ªæéƒ¨åˆ†
with tfmot.sparsity.keras.prune_scope():  # ç¡®ä¿å‰ªæä½œç”¨åŸŸæ­£ç¡®
    pruned_model = tfmot.sparsity.keras.prune_low_magnitude(
        prunable_model, **PRUNING_PARAMS
    )

# ğŸ”¹ é‡æ–°ç»„åˆæ¨¡å‹
final_model = tf.keras.Sequential([
    rescale_layer,  # å‰ç½® Rescaling
    pruned_model    # å‰ªæåçš„æ¨¡å‹éƒ¨åˆ†
])

# ç¼–è¯‘æ¨¡å‹
final_model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001),
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
    metrics=['accuracy']
)
# ğŸ”¹ æ•°æ®å¢å¼º
data_augmentation = tf.keras.Sequential([
    tf.keras.layers.RandomFlip("horizontal"),
    tf.keras.layers.RandomRotation(factor=(-0.125,0.125),fill_mode="nearest"),
    tf.keras.layers.RandomZoom(0.25,fill_mode="nearest"),
    tf.keras.layers.RandomTranslation(height_factor=0.25, width_factor=0.25),
    tf.keras.layers.RandomBrightness(0.25),
    tf.keras.layers.RandomContrast(0.3)
])

# é¢„å¤„ç†å‡½æ•°ï¼ˆæ·»åŠ å¢å¼ºï¼‰
def preprocess_image_aug(image, label):
    image = data_augmentation(image)
    return image, label
# åŠ è½½æ•°æ®é›†ï¼ˆä½¿ç”¨æ–°æ•°æ®é›†ï¼‰
base_dir = './dataset'
train_dir = os.path.join(base_dir, '80')
valid_dir = os.path.join(base_dir, '80')

BATCH_SIZE = 64
train_dataset_raw = tf.keras.preprocessing.image_dataset_from_directory(
    train_dir, validation_split=0.2, subset="training", seed=12,
    batch_size=BATCH_SIZE, image_size=IMG_SIZE)

validation_dataset_raw = tf.keras.preprocessing.image_dataset_from_directory(
    valid_dir, validation_split=0.2, subset="validation", seed=12,
    batch_size=BATCH_SIZE, image_size=IMG_SIZE)

# æ•°æ®é¢„å¤„ç†ï¼ˆåº”ç”¨å¢å¼ºï¼‰
train_dataset = (train_dataset_raw
                 .map(preprocess_image_aug, num_parallel_calls=AUTOTUNE)
                 .cache()
                 .shuffle(1000)
                 .prefetch(AUTOTUNE))

validation_dataset = (validation_dataset_raw
                      .map(preprocess_image, num_parallel_calls=AUTOTUNE)
                      .cache()
                      .prefetch(AUTOTUNE))
# ğŸ”¹ æ·»åŠ å‰ªæå›è°ƒ
pruning_callbacks = [
    tfmot.sparsity.keras.UpdatePruningStep(),
    tfmot.sparsity.keras.PruningSummaries(log_dir='./logs_pruning')
]
# è®­ç»ƒç¬¬ä¸‰é˜¶æ®µ
history_stage3 = model.fit(
    train_dataset,
    validation_data=validation_dataset,
    epochs=100,
    callbacks=[early_stopping, pruning_callbacks]
)

# å»é™¤å‰ªæåŒ…è£…
final_model = tfmot.sparsity.keras.strip_pruning(model)
final_model.save('./model/stage3_pruned_final.h5')
# åŠ è½½å‰ªæåçš„æ¨¡å‹
model = tf.keras.models.load_model('./model/stage3_pruned_final.h5')

def representative_dataset():
    for image_batch, _ in tqdm(validation_dataset_raw.take(500), desc="Processing"):
        yield [tf.cast(image_batch, tf.float32)]

converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_dataset
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.uint8
converter.inference_output_type = tf.uint8
tflite_model_quant = converter.convert()

# ---- 4. åŠ¨æ€ç”Ÿæˆå¸¦æ—¶é—´çš„æ–‡ä»¶å ----
timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M")
output_tflite_path = f'./model/model_{timestamp}.tflite'  # æ–°æ–‡ä»¶åæ ¼å¼

with open(output_tflite_path, 'wb') as f:
    f.write(tflite_model_quant)

target_dir = "model"
# ç›´æ¥åŒ¹é…å½“å‰ç›®å½•ä¸‹çš„ .h5 æ–‡ä»¶
for file in os.listdir(target_dir):
    if file.endswith(".h5"):
        file_path = os.path.join(target_dir, file)
        try:
            os.remove(file_path)
            print(f"å·²åˆ é™¤: {file_path}")
        except Exception as e:
            print(f"åˆ é™¤å¤±è´¥ [{file_path}]: {e}")
# æ··æ·†çŸ©é˜µ
y_pred = np.argmax(model.predict(validation_dataset), axis=1)
y_true = np.concatenate([labels.numpy() for _, labels in validation_dataset])

cm = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, cmap="Blues", fmt="d", 
            xticklabels=class_names, yticklabels=class_names)
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix")
plt.show()
from librariy import polt_improved

stage_names = ["Stage 1", "Stage 2", "Stage 3 (Pruning)"]
history_list = [history_stage1, history_stage2, history_stage3]
polt_improved.plot_combined_curves_improved(history_list)