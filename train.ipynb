{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter Notebook - ä»£ç \n",
    "\n",
    "# å¯¼å…¥å¿…è¦çš„åº“\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow_model_optimization as tfmot  # æ–°å¢å‰ªæåº“\n",
    "import datetime  # æ–°å¢æ—¶é—´æ¨¡å—\n",
    "# è®¾å®šæ—¥å¿—çº§åˆ«\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "\n",
    "# ğŸ”¹ è¶…å‚æ•°\n",
    "IMG_SIZE = (128, 128)\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------- ç¬¬ä¸€é˜¶æ®µè®­ç»ƒ ---------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2212 files belonging to 15 classes.\n",
      "Using 2210 files for training.\n",
      "Found 7878 files belonging to 15 classes.\n",
      "Using 7870 files for validation.\n",
      "Class Names: ['10_keyboard', '11_mobile_phone', '12_mouse', '13_headphones', '14_monitor', '15_speaker', '1_wrench', '2_soldering_iron', '3_electrodrill', '4_tape_measure', '5_screwdriver', '6_pliers', '7_oscillograph', '8_multimeter', '9_printer']\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”¹ æ•°æ®é›†è·¯å¾„\n",
    "base_dir = './dataset'\n",
    "train_dir = os.path.join(base_dir, 'stage1/train')\n",
    "valid_dir = os.path.join(base_dir, 'val')\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "# ğŸ”¹ åŠ è½½æ•°æ®é›†\n",
    "train_dataset_raw = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    train_dir, validation_split=0.001, subset=\"training\", seed=12,\n",
    "    batch_size=BATCH_SIZE, image_size=IMG_SIZE)\n",
    "\n",
    "validation_dataset_raw = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    valid_dir, validation_split=0.999, subset=\"validation\", seed=12,\n",
    "    batch_size=BATCH_SIZE, image_size=IMG_SIZE)\n",
    "\n",
    "class_names = train_dataset_raw.class_names\n",
    "print(\"Class Names:\", class_names)\n",
    "\n",
    "# é¢„å¤„ç†å‡½æ•°\n",
    "def preprocess_image(image, label):\n",
    "    return image, label\n",
    "\n",
    "# åŠ è½½æ•°æ®é›†\n",
    "train_dataset = (train_dataset_raw\n",
    "                 .map(preprocess_image, num_parallel_calls=AUTOTUNE)\n",
    "                 .cache()\n",
    "                 .shuffle(1000)\n",
    "                 .prefetch(AUTOTUNE))\n",
    "\n",
    "validation_dataset = (validation_dataset_raw\n",
    "                      .map(preprocess_image, num_parallel_calls=AUTOTUNE)\n",
    "                      .cache()\n",
    "                      .prefetch(AUTOTUNE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling (Rescaling)       (None, 128, 128, 3)       0         \n",
      "                                                                 \n",
      " mobilenetv2_0.35_128 (Funct  (None, 4, 4, 1280)       410208    \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 1280)             0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1280)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 15)                19215     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 429,423\n",
      "Trainable params: 415,343\n",
      "Non-trainable params: 14,080\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”¹ æ„å»ºæ¨¡å‹\n",
    "IMG_SHAPE = IMG_SIZE + (3,)\n",
    "base_model = tf.keras.applications.MobileNetV2(\n",
    "    input_shape=IMG_SHAPE, include_top=False, alpha=0.35, weights='imagenet')\n",
    "\n",
    "# å†»ç»“é™¤æœ€å4å±‚å¤–çš„æ‰€æœ‰å±‚\n",
    "base_model.trainable = True\n",
    "# for layer in base_model.layers[:-0]:\n",
    "#     layer.trainable = False\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Rescaling(1./255),\n",
    "    base_model,\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(len(class_names), activation='softmax')\n",
    "])\n",
    "model.build((None, 128, 128, 3))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç¼–è¯‘æ¨¡å‹\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.0001, decay_steps=10000, decay_rate=0.90, staircase=True)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# è®­ç»ƒç¬¬ä¸€é˜¶æ®µ\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=100, restore_best_weights=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "18/18 [==============================] - 4s 225ms/step - loss: 0.0153 - accuracy: 0.9991 - val_loss: 2.4282 - val_accuracy: 0.4855\n",
      "Epoch 2/20\n",
      "18/18 [==============================] - 4s 215ms/step - loss: 0.0115 - accuracy: 0.9995 - val_loss: 2.3941 - val_accuracy: 0.4900\n",
      "Epoch 3/20\n",
      "18/18 [==============================] - 4s 213ms/step - loss: 0.0116 - accuracy: 1.0000 - val_loss: 2.3481 - val_accuracy: 0.4950\n",
      "Epoch 4/20\n",
      "18/18 [==============================] - 4s 213ms/step - loss: 0.0121 - accuracy: 0.9991 - val_loss: 2.3298 - val_accuracy: 0.4968\n",
      "Epoch 5/20\n",
      "18/18 [==============================] - 4s 213ms/step - loss: 0.0106 - accuracy: 0.9995 - val_loss: 2.3197 - val_accuracy: 0.5000\n",
      "Epoch 6/20\n",
      "18/18 [==============================] - 4s 213ms/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 2.2615 - val_accuracy: 0.5079\n",
      "Epoch 7/20\n",
      "18/18 [==============================] - 4s 213ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 2.2116 - val_accuracy: 0.5158\n",
      "Epoch 8/20\n",
      "18/18 [==============================] - 4s 213ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 2.2043 - val_accuracy: 0.5175\n",
      "Epoch 9/20\n",
      "18/18 [==============================] - 4s 214ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 2.1801 - val_accuracy: 0.5235\n",
      "Epoch 10/20\n",
      "18/18 [==============================] - 4s 214ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 2.1463 - val_accuracy: 0.5280\n",
      "Epoch 11/20\n",
      "18/18 [==============================] - 4s 214ms/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 2.1427 - val_accuracy: 0.5306\n",
      "Epoch 12/20\n",
      "18/18 [==============================] - 4s 207ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 2.1460 - val_accuracy: 0.5316\n",
      "Epoch 13/20\n",
      "18/18 [==============================] - 4s 213ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 2.1323 - val_accuracy: 0.5366\n",
      "Epoch 14/20\n",
      "18/18 [==============================] - 4s 213ms/step - loss: 0.0057 - accuracy: 0.9995 - val_loss: 2.1105 - val_accuracy: 0.5395\n",
      "Epoch 15/20\n",
      "18/18 [==============================] - 4s 208ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 2.1154 - val_accuracy: 0.5380\n",
      "Epoch 16/20\n",
      "18/18 [==============================] - 4s 208ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 2.1267 - val_accuracy: 0.5381\n",
      "Epoch 17/20\n",
      "18/18 [==============================] - 4s 208ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 2.1230 - val_accuracy: 0.5408\n",
      "Epoch 18/20\n",
      "18/18 [==============================] - 4s 208ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 2.1266 - val_accuracy: 0.5427\n",
      "Epoch 19/20\n",
      "18/18 [==============================] - 4s 208ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 2.1301 - val_accuracy: 0.5428\n",
      "Epoch 20/20\n",
      "18/18 [==============================] - 4s 209ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 2.1246 - val_accuracy: 0.5451\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_dataset, validation_data=validation_dataset,\n",
    "                    epochs=20, callbacks=[early_stopping])\n",
    "\n",
    "# ä¿å­˜ç¬¬ä¸€é˜¶æ®µæ¨¡å‹\n",
    "model.save('./model/stage1_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------- ç¬¬äºŒé˜¶æ®µè®­ç»ƒ ---------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8766 files belonging to 15 classes.\n",
      "Using 8758 files for training.\n",
      "Found 7878 files belonging to 15 classes.\n",
      "Using 7870 files for validation.\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”¹ æ•°æ®é›†è·¯å¾„\n",
    "base_dir = './dataset'\n",
    "train_dir = os.path.join(base_dir, 'stage2/train')\n",
    "valid_dir = os.path.join(base_dir, 'val')\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# åŠ è½½æ•°æ®é›†\n",
    "train_dataset_raw = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    train_dir, validation_split=0.001, subset=\"training\", seed=12,\n",
    "    batch_size=BATCH_SIZE, image_size=IMG_SIZE)\n",
    "\n",
    "validation_dataset_raw = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    valid_dir, validation_split=0.999, subset=\"validation\", seed=12,\n",
    "    batch_size=BATCH_SIZE, image_size=IMG_SIZE)\n",
    "\n",
    "# æ•°æ®é¢„å¤„ç†ï¼ˆåŒç¬¬ä¸€é˜¶æ®µï¼‰\n",
    "train_dataset = (train_dataset_raw\n",
    "                 .map(preprocess_image, num_parallel_calls=AUTOTUNE)\n",
    "                 .cache()\n",
    "                 .shuffle(1000)\n",
    "                 .prefetch(AUTOTUNE))\n",
    "\n",
    "validation_dataset = (validation_dataset_raw\n",
    "                      .map(preprocess_image, num_parallel_calls=AUTOTUNE)\n",
    "                      .cache()\n",
    "                      .prefetch(AUTOTUNE))\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "# åŠ è½½ç¬¬ä¸€é˜¶æ®µæ¨¡å‹\n",
    "model = tf.keras.models.load_model('./model/stage1_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "69/69 [==============================] - 23s 235ms/step - loss: 0.4482 - accuracy: 0.8592 - val_loss: 1.2432 - val_accuracy: 0.6309\n",
      "Epoch 2/100\n",
      "69/69 [==============================] - 9s 125ms/step - loss: 0.1373 - accuracy: 0.9581 - val_loss: 0.9603 - val_accuracy: 0.7122\n",
      "Epoch 3/100\n",
      "69/69 [==============================] - 9s 125ms/step - loss: 0.0755 - accuracy: 0.9794 - val_loss: 0.8784 - val_accuracy: 0.7388\n",
      "Epoch 4/100\n",
      "69/69 [==============================] - 9s 125ms/step - loss: 0.0459 - accuracy: 0.9895 - val_loss: 0.7614 - val_accuracy: 0.7801\n",
      "Epoch 5/100\n",
      "69/69 [==============================] - 9s 125ms/step - loss: 0.0316 - accuracy: 0.9938 - val_loss: 0.7035 - val_accuracy: 0.7999\n",
      "Epoch 6/100\n",
      "69/69 [==============================] - 9s 126ms/step - loss: 0.0215 - accuracy: 0.9969 - val_loss: 0.6892 - val_accuracy: 0.8061\n",
      "Epoch 7/100\n",
      "69/69 [==============================] - 9s 126ms/step - loss: 0.0166 - accuracy: 0.9978 - val_loss: 0.6665 - val_accuracy: 0.8160\n",
      "Epoch 8/100\n",
      "69/69 [==============================] - 9s 125ms/step - loss: 0.0128 - accuracy: 0.9994 - val_loss: 0.6894 - val_accuracy: 0.8174\n",
      "Epoch 9/100\n",
      "69/69 [==============================] - 9s 125ms/step - loss: 0.0104 - accuracy: 0.9994 - val_loss: 0.7044 - val_accuracy: 0.8180\n",
      "Epoch 10/100\n",
      "69/69 [==============================] - 9s 127ms/step - loss: 0.0082 - accuracy: 0.9994 - val_loss: 0.6648 - val_accuracy: 0.8276\n",
      "Epoch 11/100\n",
      "69/69 [==============================] - 9s 127ms/step - loss: 0.0072 - accuracy: 0.9995 - val_loss: 0.6426 - val_accuracy: 0.8339\n",
      "Epoch 12/100\n",
      "69/69 [==============================] - 9s 127ms/step - loss: 0.0061 - accuracy: 0.9997 - val_loss: 0.6230 - val_accuracy: 0.8407\n",
      "Epoch 13/100\n",
      "69/69 [==============================] - 9s 125ms/step - loss: 0.0056 - accuracy: 0.9997 - val_loss: 0.6400 - val_accuracy: 0.8366\n",
      "Epoch 14/100\n",
      "69/69 [==============================] - 9s 126ms/step - loss: 0.0045 - accuracy: 0.9998 - val_loss: 0.6216 - val_accuracy: 0.8445\n",
      "Epoch 15/100\n",
      "69/69 [==============================] - 9s 127ms/step - loss: 0.0039 - accuracy: 0.9999 - val_loss: 0.6149 - val_accuracy: 0.8478\n",
      "Epoch 16/100\n",
      "69/69 [==============================] - 9s 127ms/step - loss: 0.0036 - accuracy: 0.9999 - val_loss: 0.6087 - val_accuracy: 0.8470\n",
      "Epoch 17/100\n",
      "69/69 [==============================] - 9s 127ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.5938 - val_accuracy: 0.8518\n",
      "Epoch 18/100\n",
      "69/69 [==============================] - 9s 128ms/step - loss: 0.0029 - accuracy: 0.9999 - val_loss: 0.5852 - val_accuracy: 0.8550\n",
      "Epoch 19/100\n",
      "69/69 [==============================] - 9s 128ms/step - loss: 0.0025 - accuracy: 0.9999 - val_loss: 0.5756 - val_accuracy: 0.8598\n",
      "Epoch 20/100\n",
      "69/69 [==============================] - 9s 127ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.5724 - val_accuracy: 0.8611\n",
      "Epoch 21/100\n",
      "69/69 [==============================] - 9s 126ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.5756 - val_accuracy: 0.8625\n",
      "Epoch 22/100\n",
      "69/69 [==============================] - 9s 125ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.5850 - val_accuracy: 0.8600\n",
      "Epoch 23/100\n",
      "69/69 [==============================] - 9s 126ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.5954 - val_accuracy: 0.8595\n",
      "Epoch 24/100\n",
      "69/69 [==============================] - 9s 126ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.5943 - val_accuracy: 0.8604\n",
      "Epoch 25/100\n",
      "69/69 [==============================] - 9s 125ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.5790 - val_accuracy: 0.8637\n",
      "Epoch 26/100\n",
      "69/69 [==============================] - 9s 126ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.5820 - val_accuracy: 0.8635\n",
      "Epoch 27/100\n",
      "69/69 [==============================] - 9s 125ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.5873 - val_accuracy: 0.8638\n",
      "Epoch 28/100\n",
      "69/69 [==============================] - 9s 126ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.5824 - val_accuracy: 0.8668\n",
      "Epoch 29/100\n",
      "69/69 [==============================] - 9s 127ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.5905 - val_accuracy: 0.8653\n",
      "Epoch 30/100\n",
      "69/69 [==============================] - 9s 128ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.5905 - val_accuracy: 0.8680\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_dataset, validation_data=validation_dataset,\n",
    "                    epochs=100, callbacks=[early_stopping])\n",
    "\n",
    "# ä¿å­˜ç¬¬ä¸€é˜¶æ®µæ¨¡å‹\n",
    "model.save('./model/stage2_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Could not find directory ./dataset\\stagee/train",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m BATCH_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# åŠ è½½æ•°æ®é›†\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m train_dataset_raw \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocessing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_dataset_from_directory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mIMG_SIZE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m validation_dataset_raw \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mpreprocessing\u001b[38;5;241m.\u001b[39mimage_dataset_from_directory(\n\u001b[0;32m     14\u001b[0m     valid_dir, validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.999\u001b[39m, subset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m,\n\u001b[0;32m     15\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE, image_size\u001b[38;5;241m=\u001b[39mIMG_SIZE)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# æ•°æ®é¢„å¤„ç†ï¼ˆåŒç¬¬ä¸€é˜¶æ®µï¼‰\u001b[39;00m\n",
      "File \u001b[1;32md:\\miniconda\\envs\\tf\\lib\\site-packages\\keras\\utils\\image_dataset.py:207\u001b[0m, in \u001b[0;36mimage_dataset_from_directory\u001b[1;34m(directory, labels, label_mode, class_names, color_mode, batch_size, image_size, shuffle, seed, validation_split, subset, interpolation, follow_links, crop_to_aspect_ratio, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m seed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     seed \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m1e6\u001b[39m)\n\u001b[1;32m--> 207\u001b[0m image_paths, labels, class_names \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex_directory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[43m    \u001b[49m\u001b[43mformats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mALLOWLIST_FORMATS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_links\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_links\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m label_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(class_names) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWhen passing `label_mode=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`, there must be exactly 2 \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass_names. Received: class_names=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    221\u001b[0m     )\n",
      "File \u001b[1;32md:\\miniconda\\envs\\tf\\lib\\site-packages\\keras\\utils\\dataset_utils.py:524\u001b[0m, in \u001b[0;36mindex_directory\u001b[1;34m(directory, labels, formats, class_names, shuffle, seed, follow_links)\u001b[0m\n\u001b[0;32m    522\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    523\u001b[0m     subdirs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 524\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m subdir \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[0;32m    525\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39misdir(tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mjoin(directory, subdir)):\n\u001b[0;32m    526\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m subdir\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32md:\\miniconda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py:766\u001b[0m, in \u001b[0;36mlist_directory_v2\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    751\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns a list of entries contained within a directory.\u001b[39;00m\n\u001b[0;32m    752\u001b[0m \n\u001b[0;32m    753\u001b[0m \u001b[38;5;124;03mThe list is in arbitrary order. It does not contain the special entries \".\"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    763\u001b[0m \u001b[38;5;124;03m  errors.NotFoundError if directory doesn't exist\u001b[39;00m\n\u001b[0;32m    764\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    765\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_directory(path):\n\u001b[1;32m--> 766\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mNotFoundError(\n\u001b[0;32m    767\u001b[0m       node_def\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    768\u001b[0m       op\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    769\u001b[0m       message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find directory \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(path))\n\u001b[0;32m    771\u001b[0m \u001b[38;5;66;03m# Convert each element to string, since the return values of the\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;66;03m# vector of string should be interpreted as strings, not bytes.\u001b[39;00m\n\u001b[0;32m    773\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    774\u001b[0m     compat\u001b[38;5;241m.\u001b[39mas_str_any(filename)\n\u001b[0;32m    775\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m _pywrap_file_io\u001b[38;5;241m.\u001b[39mGetChildren(compat\u001b[38;5;241m.\u001b[39mpath_to_bytes(path))\n\u001b[0;32m    776\u001b[0m ]\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Could not find directory ./dataset\\stagee/train"
     ]
    }
   ],
   "source": [
    "# ğŸ”¹ æ•°æ®é›†è·¯å¾„\n",
    "base_dir = './dataset'\n",
    "train_dir = os.path.join(base_dir, 'stage4/train')\n",
    "valid_dir = os.path.join(base_dir, 'val')\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# åŠ è½½æ•°æ®é›†\n",
    "train_dataset_raw = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    train_dir, validation_split=0.001, subset=\"training\", seed=12,\n",
    "    batch_size=BATCH_SIZE, image_size=IMG_SIZE)\n",
    "\n",
    "validation_dataset_raw = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    valid_dir, validation_split=0.999, subset=\"validation\", seed=12,\n",
    "    batch_size=BATCH_SIZE, image_size=IMG_SIZE)\n",
    "\n",
    "# æ•°æ®é¢„å¤„ç†ï¼ˆåŒç¬¬ä¸€é˜¶æ®µï¼‰\n",
    "train_dataset = (train_dataset_raw\n",
    "                 .map(preprocess_image, num_parallel_calls=AUTOTUNE)\n",
    "                 .cache()\n",
    "                 .shuffle(1000)\n",
    "                 .prefetch(AUTOTUNE))\n",
    "\n",
    "validation_dataset = (validation_dataset_raw\n",
    "                      .map(preprocess_image, num_parallel_calls=AUTOTUNE)\n",
    "                      .cache()\n",
    "                      .prefetch(AUTOTUNE))\n",
    "\n",
    "# åŠ è½½ç¬¬ä¸€é˜¶æ®µæ¨¡å‹\n",
    "model = tf.keras.models.load_model('./model/stage2_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "274/274 [==============================] - 40s 103ms/step - loss: 0.1559 - accuracy: 0.9503 - val_loss: 0.6731 - val_accuracy: 0.8384\n",
      "Epoch 2/100\n",
      "274/274 [==============================] - 20s 71ms/step - loss: 0.0377 - accuracy: 0.9884 - val_loss: 0.5535 - val_accuracy: 0.8666\n",
      "Epoch 3/100\n",
      "274/274 [==============================] - 19s 71ms/step - loss: 0.0174 - accuracy: 0.9961 - val_loss: 0.6020 - val_accuracy: 0.8659\n",
      "Epoch 4/100\n",
      "274/274 [==============================] - 20s 73ms/step - loss: 0.0103 - accuracy: 0.9983 - val_loss: 0.5267 - val_accuracy: 0.8854\n",
      "Epoch 5/100\n",
      "274/274 [==============================] - 19s 70ms/step - loss: 0.0080 - accuracy: 0.9987 - val_loss: 0.6053 - val_accuracy: 0.8738\n",
      "Epoch 6/100\n",
      "274/274 [==============================] - 20s 74ms/step - loss: 0.0052 - accuracy: 0.9994 - val_loss: 0.4461 - val_accuracy: 0.9011\n",
      "Epoch 7/100\n",
      "274/274 [==============================] - 20s 72ms/step - loss: 0.0037 - accuracy: 0.9995 - val_loss: 0.4504 - val_accuracy: 0.9014\n",
      "Epoch 8/100\n",
      "274/274 [==============================] - 19s 71ms/step - loss: 0.0028 - accuracy: 0.9997 - val_loss: 0.5680 - val_accuracy: 0.8870\n",
      "Epoch 9/100\n",
      "274/274 [==============================] - 20s 71ms/step - loss: 0.0022 - accuracy: 0.9997 - val_loss: 0.5010 - val_accuracy: 0.9019\n",
      "Epoch 10/100\n",
      "274/274 [==============================] - 19s 70ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.5578 - val_accuracy: 0.8940\n",
      "Epoch 11/100\n",
      "274/274 [==============================] - 20s 72ms/step - loss: 0.0013 - accuracy: 0.9999 - val_loss: 0.5352 - val_accuracy: 0.8982\n",
      "Epoch 12/100\n",
      "274/274 [==============================] - 19s 70ms/step - loss: 0.0033 - accuracy: 0.9993 - val_loss: 0.6019 - val_accuracy: 0.8879\n",
      "Epoch 13/100\n",
      "274/274 [==============================] - 20s 71ms/step - loss: 0.0097 - accuracy: 0.9975 - val_loss: 0.8506 - val_accuracy: 0.8562\n",
      "Epoch 14/100\n",
      "274/274 [==============================] - 19s 71ms/step - loss: 0.0059 - accuracy: 0.9985 - val_loss: 0.6010 - val_accuracy: 0.8887\n",
      "Epoch 15/100\n",
      "274/274 [==============================] - 19s 71ms/step - loss: 0.0033 - accuracy: 0.9993 - val_loss: 0.5884 - val_accuracy: 0.8900\n",
      "Epoch 16/100\n",
      "274/274 [==============================] - 20s 72ms/step - loss: 0.0021 - accuracy: 0.9996 - val_loss: 0.6359 - val_accuracy: 0.8859\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_dataset, validation_data=validation_dataset,\n",
    "                    epochs=100, callbacks=[early_stopping])\n",
    "\n",
    "# ä¿å­˜ç¬¬ä¸€é˜¶æ®µæ¨¡å‹\n",
    "model.save('./model/stage2_model.h5')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------- ç¬¬ä¸‰é˜¶æ®µè®­ç»ƒ ---------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”¹ å‰ªæå‚æ•°é…ç½®\n",
    "PRUNING_PARAMS = {\n",
    "    'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(\n",
    "        initial_sparsity=0.30,\n",
    "        final_sparsity=0.60,\n",
    "        begin_step=0,\n",
    "        end_step=2000,\n",
    "        frequency=100\n",
    "    )\n",
    "}\n",
    "\n",
    "# åŠ è½½ç¬¬äºŒé˜¶æ®µæ¨¡å‹\n",
    "model = tf.keras.models.load_model('./model/stage2_model.h5')\n",
    "\n",
    "# ğŸ”¹ åˆ†ç¦» Rescaling å±‚å’ŒåŸºç¡€æ¨¡å‹\n",
    "rescale_layer = model.layers[0]  # æå– Rescaling å±‚\n",
    "prunable_model = tf.keras.Sequential(model.layers[1:])  # æ’é™¤ Rescaling åçš„æ¨¡å‹\n",
    "\n",
    "# ğŸ”¹ åº”ç”¨å‰ªæåˆ°å¯å‰ªæéƒ¨åˆ†\n",
    "with tfmot.sparsity.keras.prune_scope():  # ç¡®ä¿å‰ªæä½œç”¨åŸŸæ­£ç¡®\n",
    "    pruned_model = tfmot.sparsity.keras.prune_low_magnitude(\n",
    "        prunable_model, **PRUNING_PARAMS\n",
    "    )\n",
    "\n",
    "# ğŸ”¹ é‡æ–°ç»„åˆæ¨¡å‹\n",
    "final_model = tf.keras.Sequential([\n",
    "    rescale_layer,  # å‰ç½® Rescaling\n",
    "    pruned_model    # å‰ªæåçš„æ¨¡å‹éƒ¨åˆ†\n",
    "])\n",
    "\n",
    "# ç¼–è¯‘æ¨¡å‹\n",
    "final_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.00005),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”¹ æ•°æ®å¢å¼º\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "    tf.keras.layers.RandomRotation(0.125),\n",
    "    tf.keras.layers.RandomZoom(0.5),\n",
    "    tf.keras.layers.RandomTranslation(height_factor=0.25, width_factor=0.25),\n",
    "    tf.keras.layers.RandomBrightness(0.25),\n",
    "    tf.keras.layers.RandomContrast(0.5)\n",
    "])\n",
    "\n",
    "# é¢„å¤„ç†å‡½æ•°ï¼ˆæ·»åŠ å¢å¼ºï¼‰\n",
    "def preprocess_image_aug(image, label):\n",
    "    image = data_augmentation(image)\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 27690 files belonging to 15 classes.\n",
      "Using 22152 files for training.\n",
      "Found 7878 files belonging to 15 classes.\n",
      "Using 1575 files for validation.\n"
     ]
    }
   ],
   "source": [
    "# åŠ è½½æ•°æ®é›†ï¼ˆä½¿ç”¨æ–°æ•°æ®é›†ï¼‰\n",
    "base_dir = './dataset'\n",
    "train_dir = os.path.join(base_dir, 'stage4/train')\n",
    "valid_dir = os.path.join(base_dir, 'val')\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "train_dataset_raw = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    train_dir, validation_split=0.001, subset=\"training\", seed=12,\n",
    "    batch_size=BATCH_SIZE, image_size=IMG_SIZE)\n",
    "\n",
    "validation_dataset_raw = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    valid_dir, validation_split=0.999, subset=\"validation\", seed=12,\n",
    "    batch_size=BATCH_SIZE, image_size=IMG_SIZE)\n",
    "\n",
    "# æ•°æ®é¢„å¤„ç†ï¼ˆåº”ç”¨å¢å¼ºï¼‰\n",
    "train_dataset = (train_dataset_raw\n",
    "                 .map(preprocess_image_aug, num_parallel_calls=AUTOTUNE)\n",
    "                 .cache()\n",
    "                 .shuffle(1000)\n",
    "                 .prefetch(AUTOTUNE))\n",
    "\n",
    "validation_dataset = (validation_dataset_raw\n",
    "                      .map(preprocess_image, num_parallel_calls=AUTOTUNE)\n",
    "                      .cache()\n",
    "                      .prefetch(AUTOTUNE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”¹ æ·»åŠ å‰ªæå›è°ƒ\n",
    "pruning_callbacks = [\n",
    "    tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "    tfmot.sparsity.keras.PruningSummaries(log_dir='./logs_pruning')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "347/347 [==============================] - 52s 74ms/step - loss: 0.0134 - accuracy: 0.9953 - val_loss: 0.5527 - val_accuracy: 0.8978\n",
      "Epoch 2/100\n",
      "347/347 [==============================] - 22s 63ms/step - loss: 0.0059 - accuracy: 0.9987 - val_loss: 0.4400 - val_accuracy: 0.9156\n",
      "Epoch 3/100\n",
      "347/347 [==============================] - 22s 63ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.4060 - val_accuracy: 0.9181\n",
      "Epoch 4/100\n",
      "347/347 [==============================] - 22s 63ms/step - loss: 7.2651e-04 - accuracy: 1.0000 - val_loss: 0.4017 - val_accuracy: 0.9225\n",
      "Epoch 5/100\n",
      "347/347 [==============================] - 22s 63ms/step - loss: 6.2278e-04 - accuracy: 1.0000 - val_loss: 0.4023 - val_accuracy: 0.9276\n",
      "Epoch 6/100\n",
      "347/347 [==============================] - 22s 64ms/step - loss: 4.1497e-04 - accuracy: 1.0000 - val_loss: 0.3584 - val_accuracy: 0.9314\n",
      "Epoch 7/100\n",
      "347/347 [==============================] - 23s 65ms/step - loss: 3.6961e-04 - accuracy: 1.0000 - val_loss: 0.3645 - val_accuracy: 0.9308\n",
      "Epoch 8/100\n",
      "347/347 [==============================] - 22s 64ms/step - loss: 3.3706e-04 - accuracy: 1.0000 - val_loss: 0.3659 - val_accuracy: 0.9327\n",
      "Epoch 9/100\n",
      "347/347 [==============================] - 22s 64ms/step - loss: 0.0029 - accuracy: 0.9992 - val_loss: 0.6199 - val_accuracy: 0.8952\n",
      "Epoch 10/100\n",
      "347/347 [==============================] - 23s 65ms/step - loss: 0.0090 - accuracy: 0.9972 - val_loss: 0.5472 - val_accuracy: 0.8940\n",
      "Epoch 11/100\n",
      "347/347 [==============================] - 22s 64ms/step - loss: 0.0024 - accuracy: 0.9992 - val_loss: 0.4229 - val_accuracy: 0.9130\n",
      "Epoch 12/100\n",
      "339/347 [============================>.] - ETA: 0s - loss: 0.0017 - accuracy: 0.9994"
     ]
    }
   ],
   "source": [
    "# è®­ç»ƒç¬¬ä¸‰é˜¶æ®µ\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=validation_dataset,\n",
    "    epochs=100,\n",
    "    callbacks=[early_stopping, pruning_callbacks]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å»é™¤å‰ªæåŒ…è£…\n",
    "final_model = tfmot.sparsity.keras.strip_pruning(model)\n",
    "final_model.save('./model/stage3_pruned_final.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŠ è½½å‰ªæåçš„æ¨¡å‹\n",
    "model = tf.keras.models.load_model('./model/stage3_pruned_final.h5')\n",
    "\n",
    "def representative_dataset():\n",
    "    for image_batch, _ in tqdm(validation_dataset.take(500), desc=\"Processing\"):\n",
    "        yield [tf.cast(image_batch, tf.float32)]\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "tflite_model_quant = converter.convert()\n",
    "\n",
    "# ---- 4. åŠ¨æ€ç”Ÿæˆå¸¦æ—¶é—´çš„æ–‡ä»¶å ----\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_tflite_path = f'./model/model_{timestamp}.tflite'  # æ–°æ–‡ä»¶åæ ¼å¼\n",
    "\n",
    "with open(output_tflite_path, 'wb') as f:\n",
    "    f.write(tflite_model_quant)\n",
    "\n",
    "target_dir = \"model\"\n",
    "# ç›´æ¥åŒ¹é…å½“å‰ç›®å½•ä¸‹çš„ .h5 æ–‡ä»¶\n",
    "for file in os.listdir(target_dir):\n",
    "    if file.endswith(\".h5\"):\n",
    "        file_path = os.path.join(target_dir, file)\n",
    "        try:\n",
    "            os.remove(file_path)\n",
    "            print(f\"å·²åˆ é™¤: {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"åˆ é™¤å¤±è´¥ [{file_path}]: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ··æ·†çŸ©é˜µ\n",
    "y_pred = np.argmax(model.predict(validation_dataset), axis=1)\n",
    "y_true = np.concatenate([labels.numpy() for _, labels in validation_dataset])\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, cmap=\"Blues\", fmt=\"d\", \n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
