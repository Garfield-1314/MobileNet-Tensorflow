{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter Notebook - ‰ª£Á†Å\n",
    "\n",
    "# ÂØºÂÖ•ÂøÖË¶ÅÁöÑÂ∫ì\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow_model_optimization as tfmot  # Êñ∞Â¢ûÂâ™ÊûùÂ∫ì\n",
    "import datetime\n",
    "# ËÆæÂÆöÊó•ÂøóÁ∫ßÂà´\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "# üîπ Ë∂ÖÂèÇÊï∞\n",
    "IMG_SIZE = (128, 128)\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------- Á¨¨‰∏ÄÈò∂ÊÆµËÆ≠ÁªÉ ---------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 27477 files belonging to 15 classes.\n",
      "Using 21982 files for training.\n",
      "Found 27477 files belonging to 15 classes.\n",
      "Using 5495 files for validation.\n",
      "Class Names: ['10_keyboard', '11_mobile_phone', '12_mouse', '13_headphones', '14_monitor', '15_speaker', '1_wrench', '2_soldering_iron', '3_electrodrill', '4_tape_measure', '5_screwdriver', '6_pliers', '7_oscillograph', '8_multimeter', '9_printer']\n"
     ]
    }
   ],
   "source": [
    "# üîπ Êï∞ÊçÆÈõÜË∑ØÂæÑ\n",
    "base_dir = './dataset'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "valid_dir = os.path.join(base_dir, 'train')\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "# üîπ Âä†ËΩΩÊï∞ÊçÆÈõÜ\n",
    "train_dataset_raw = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    train_dir, validation_split=0.2, subset=\"training\", seed=12,\n",
    "    batch_size=BATCH_SIZE, image_size=IMG_SIZE)\n",
    "\n",
    "validation_dataset_raw = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    valid_dir, validation_split=0.2, subset=\"validation\", seed=12,\n",
    "    batch_size=BATCH_SIZE, image_size=IMG_SIZE)\n",
    "\n",
    "class_names = train_dataset_raw.class_names\n",
    "print(\"Class Names:\", class_names)\n",
    "\n",
    "# È¢ÑÂ§ÑÁêÜÂáΩÊï∞\n",
    "def preprocess_image(image, label):\n",
    "    return image, label\n",
    "\n",
    "# Âä†ËΩΩÊï∞ÊçÆÈõÜ\n",
    "train_dataset = (train_dataset_raw\n",
    "                 .map(preprocess_image, num_parallel_calls=AUTOTUNE)\n",
    "                 .cache()\n",
    "                 .shuffle(1000)\n",
    "                 .prefetch(AUTOTUNE))\n",
    "\n",
    "validation_dataset = (validation_dataset_raw\n",
    "                      .map(preprocess_image, num_parallel_calls=AUTOTUNE)\n",
    "                      .cache()\n",
    "                      .prefetch(AUTOTUNE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling (Rescaling)       (None, 128, 128, 3)       0         \n",
      "                                                                 \n",
      " mobilenetv2_0.35_128 (Funct  (None, 1280)             410208    \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 15)                19215     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 429,423\n",
      "Trainable params: 415,343\n",
      "Non-trainable params: 14,080\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# üîπ ÊûÑÂª∫Ê®°Âûã\n",
    "IMG_SHAPE = IMG_SIZE + (3,)\n",
    "base_model = tf.keras.applications.MobileNetV2(\n",
    "    input_shape=IMG_SHAPE, include_top=False, pooling = 'avg', alpha=0.35, weights='imagenet')\n",
    "\n",
    "# ÂÜªÁªìÈô§ÊúÄÂêé4Â±ÇÂ§ñÁöÑÊâÄÊúâÂ±Ç\n",
    "base_model.trainable = True\n",
    "# for layer in base_model.layers[:-0]:\n",
    "#     layer.trainable = False\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Rescaling(1./255),\n",
    "    base_model,\n",
    "    # tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    # tf.keras.layers.Dropout(0.1),\n",
    "    tf.keras.layers.Dense(len(class_names), activation='softmax')\n",
    "])\n",
    "model.build((None, 128, 128, 3))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "172/172 [==============================] - 34s 124ms/step - loss: 2.5387 - accuracy: 0.2005 - val_loss: 2.4534 - val_accuracy: 0.2082\n",
      "Epoch 2/100\n",
      "172/172 [==============================] - 19s 110ms/step - loss: 1.7897 - accuracy: 0.4692 - val_loss: 1.8885 - val_accuracy: 0.4064\n",
      "Epoch 3/100\n",
      "172/172 [==============================] - 19s 111ms/step - loss: 1.3405 - accuracy: 0.6206 - val_loss: 1.5315 - val_accuracy: 0.5176\n",
      "Epoch 4/100\n",
      "172/172 [==============================] - 19s 111ms/step - loss: 1.0708 - accuracy: 0.6978 - val_loss: 1.2828 - val_accuracy: 0.5933\n",
      "Epoch 5/100\n",
      "172/172 [==============================] - 19s 112ms/step - loss: 0.8950 - accuracy: 0.7432 - val_loss: 1.0956 - val_accuracy: 0.6539\n",
      "Epoch 6/100\n",
      "172/172 [==============================] - 19s 112ms/step - loss: 0.7710 - accuracy: 0.7780 - val_loss: 0.9638 - val_accuracy: 0.6921\n",
      "Epoch 7/100\n",
      "172/172 [==============================] - 19s 113ms/step - loss: 0.6828 - accuracy: 0.8021 - val_loss: 0.8542 - val_accuracy: 0.7283\n",
      "Epoch 8/100\n",
      "172/172 [==============================] - 19s 113ms/step - loss: 0.6154 - accuracy: 0.8229 - val_loss: 0.7678 - val_accuracy: 0.7583\n",
      "Epoch 9/100\n",
      "172/172 [==============================] - 19s 113ms/step - loss: 0.5599 - accuracy: 0.8381 - val_loss: 0.6952 - val_accuracy: 0.7809\n",
      "Epoch 10/100\n",
      "172/172 [==============================] - 20s 114ms/step - loss: 0.5130 - accuracy: 0.8516 - val_loss: 0.6331 - val_accuracy: 0.7993\n",
      "Epoch 11/100\n",
      "172/172 [==============================] - 20s 114ms/step - loss: 0.4726 - accuracy: 0.8639 - val_loss: 0.5837 - val_accuracy: 0.8153\n",
      "Epoch 12/100\n",
      "172/172 [==============================] - 20s 114ms/step - loss: 0.4376 - accuracy: 0.8735 - val_loss: 0.5401 - val_accuracy: 0.8297\n",
      "Epoch 13/100\n",
      "172/172 [==============================] - 20s 114ms/step - loss: 0.4083 - accuracy: 0.8837 - val_loss: 0.5060 - val_accuracy: 0.8415\n",
      "Epoch 14/100\n",
      "172/172 [==============================] - 20s 114ms/step - loss: 0.3828 - accuracy: 0.8908 - val_loss: 0.4734 - val_accuracy: 0.8493\n",
      "Epoch 15/100\n",
      "172/172 [==============================] - 20s 114ms/step - loss: 0.3596 - accuracy: 0.8981 - val_loss: 0.4481 - val_accuracy: 0.8570\n",
      "Epoch 16/100\n",
      "172/172 [==============================] - 20s 115ms/step - loss: 0.3382 - accuracy: 0.9040 - val_loss: 0.4252 - val_accuracy: 0.8642\n",
      "Epoch 17/100\n",
      "172/172 [==============================] - 20s 115ms/step - loss: 0.3186 - accuracy: 0.9106 - val_loss: 0.4046 - val_accuracy: 0.8712\n",
      "Epoch 18/100\n",
      "172/172 [==============================] - 20s 115ms/step - loss: 0.3004 - accuracy: 0.9157 - val_loss: 0.3891 - val_accuracy: 0.8748\n",
      "Epoch 19/100\n",
      "172/172 [==============================] - 20s 115ms/step - loss: 0.2847 - accuracy: 0.9206 - val_loss: 0.3743 - val_accuracy: 0.8808\n",
      "Epoch 20/100\n",
      "172/172 [==============================] - 20s 115ms/step - loss: 0.2702 - accuracy: 0.9255 - val_loss: 0.3612 - val_accuracy: 0.8854\n",
      "Epoch 21/100\n",
      "172/172 [==============================] - 20s 115ms/step - loss: 0.2565 - accuracy: 0.9301 - val_loss: 0.3491 - val_accuracy: 0.8903\n",
      "Epoch 22/100\n",
      "172/172 [==============================] - 20s 115ms/step - loss: 0.2437 - accuracy: 0.9339 - val_loss: 0.3377 - val_accuracy: 0.8926\n",
      "Epoch 23/100\n",
      "172/172 [==============================] - 20s 115ms/step - loss: 0.2315 - accuracy: 0.9380 - val_loss: 0.3272 - val_accuracy: 0.8965\n",
      "Epoch 24/100\n",
      "172/172 [==============================] - 20s 116ms/step - loss: 0.2201 - accuracy: 0.9425 - val_loss: 0.3178 - val_accuracy: 0.8994\n",
      "Epoch 25/100\n",
      "172/172 [==============================] - 20s 115ms/step - loss: 0.2101 - accuracy: 0.9460 - val_loss: 0.3102 - val_accuracy: 0.9015\n",
      "Epoch 26/100\n",
      "172/172 [==============================] - 20s 115ms/step - loss: 0.2007 - accuracy: 0.9490 - val_loss: 0.3028 - val_accuracy: 0.9030\n",
      "Epoch 27/100\n",
      "172/172 [==============================] - 20s 116ms/step - loss: 0.1916 - accuracy: 0.9519 - val_loss: 0.2963 - val_accuracy: 0.9061\n",
      "Epoch 28/100\n",
      "172/172 [==============================] - 20s 117ms/step - loss: 0.1830 - accuracy: 0.9555 - val_loss: 0.2900 - val_accuracy: 0.9072\n",
      "Epoch 29/100\n",
      "172/172 [==============================] - 20s 118ms/step - loss: 0.1748 - accuracy: 0.9586 - val_loss: 0.2836 - val_accuracy: 0.9101\n",
      "Epoch 30/100\n",
      "172/172 [==============================] - 20s 116ms/step - loss: 0.1671 - accuracy: 0.9617 - val_loss: 0.2784 - val_accuracy: 0.9125\n",
      "Epoch 31/100\n",
      "172/172 [==============================] - 20s 117ms/step - loss: 0.1602 - accuracy: 0.9642 - val_loss: 0.2737 - val_accuracy: 0.9136\n",
      "Epoch 32/100\n",
      "172/172 [==============================] - 20s 117ms/step - loss: 0.1537 - accuracy: 0.9660 - val_loss: 0.2689 - val_accuracy: 0.9157\n",
      "Epoch 33/100\n",
      "172/172 [==============================] - 21s 121ms/step - loss: 0.1474 - accuracy: 0.9682 - val_loss: 0.2643 - val_accuracy: 0.9172\n",
      "Epoch 34/100\n",
      "172/172 [==============================] - 21s 121ms/step - loss: 0.1413 - accuracy: 0.9698 - val_loss: 0.2600 - val_accuracy: 0.9177\n",
      "Epoch 35/100\n",
      "172/172 [==============================] - 21s 125ms/step - loss: 0.1355 - accuracy: 0.9718 - val_loss: 0.2557 - val_accuracy: 0.9196\n",
      "Epoch 36/100\n",
      "172/172 [==============================] - 22s 126ms/step - loss: 0.1300 - accuracy: 0.9733 - val_loss: 0.2520 - val_accuracy: 0.9205\n",
      "Epoch 37/100\n",
      "172/172 [==============================] - 21s 120ms/step - loss: 0.1251 - accuracy: 0.9749 - val_loss: 0.2486 - val_accuracy: 0.9216\n",
      "Epoch 38/100\n",
      "172/172 [==============================] - 21s 121ms/step - loss: 0.1204 - accuracy: 0.9771 - val_loss: 0.2453 - val_accuracy: 0.9236\n",
      "Epoch 39/100\n",
      "172/172 [==============================] - 20s 118ms/step - loss: 0.1158 - accuracy: 0.9787 - val_loss: 0.2420 - val_accuracy: 0.9241\n",
      "Epoch 40/100\n",
      "172/172 [==============================] - 21s 123ms/step - loss: 0.1113 - accuracy: 0.9801 - val_loss: 0.2389 - val_accuracy: 0.9247\n",
      "Epoch 41/100\n",
      "172/172 [==============================] - 21s 119ms/step - loss: 0.1071 - accuracy: 0.9812 - val_loss: 0.2360 - val_accuracy: 0.9252\n",
      "Epoch 42/100\n",
      "172/172 [==============================] - 21s 121ms/step - loss: 0.1030 - accuracy: 0.9828 - val_loss: 0.2332 - val_accuracy: 0.9263\n",
      "Epoch 43/100\n",
      "172/172 [==============================] - 21s 120ms/step - loss: 0.0994 - accuracy: 0.9841 - val_loss: 0.2307 - val_accuracy: 0.9267\n",
      "Epoch 44/100\n",
      "172/172 [==============================] - 20s 118ms/step - loss: 0.0958 - accuracy: 0.9849 - val_loss: 0.2283 - val_accuracy: 0.9274\n",
      "Epoch 45/100\n",
      "172/172 [==============================] - 20s 117ms/step - loss: 0.0924 - accuracy: 0.9861 - val_loss: 0.2260 - val_accuracy: 0.9285\n",
      "Epoch 46/100\n",
      "172/172 [==============================] - 20s 119ms/step - loss: 0.0891 - accuracy: 0.9870 - val_loss: 0.2236 - val_accuracy: 0.9288\n",
      "Epoch 47/100\n",
      "172/172 [==============================] - 20s 119ms/step - loss: 0.0859 - accuracy: 0.9880 - val_loss: 0.2213 - val_accuracy: 0.9292\n",
      "Epoch 48/100\n",
      "172/172 [==============================] - 21s 120ms/step - loss: 0.0829 - accuracy: 0.9889 - val_loss: 0.2193 - val_accuracy: 0.9301\n",
      "Epoch 49/100\n",
      "172/172 [==============================] - 21s 121ms/step - loss: 0.0801 - accuracy: 0.9898 - val_loss: 0.2172 - val_accuracy: 0.9310\n",
      "Epoch 50/100\n",
      "172/172 [==============================] - 20s 119ms/step - loss: 0.0775 - accuracy: 0.9906 - val_loss: 0.2153 - val_accuracy: 0.9312\n",
      "Epoch 51/100\n",
      "172/172 [==============================] - 20s 118ms/step - loss: 0.0749 - accuracy: 0.9910 - val_loss: 0.2135 - val_accuracy: 0.9318\n",
      "Epoch 52/100\n",
      "172/172 [==============================] - 22s 125ms/step - loss: 0.0723 - accuracy: 0.9916 - val_loss: 0.2117 - val_accuracy: 0.9321\n",
      "Epoch 53/100\n",
      "172/172 [==============================] - 20s 119ms/step - loss: 0.0699 - accuracy: 0.9922 - val_loss: 0.2102 - val_accuracy: 0.9325\n",
      "Epoch 54/100\n",
      "172/172 [==============================] - 20s 118ms/step - loss: 0.0676 - accuracy: 0.9929 - val_loss: 0.2084 - val_accuracy: 0.9328\n",
      "Epoch 55/100\n",
      "172/172 [==============================] - 20s 119ms/step - loss: 0.0655 - accuracy: 0.9934 - val_loss: 0.2070 - val_accuracy: 0.9334\n",
      "Epoch 56/100\n",
      "172/172 [==============================] - 20s 118ms/step - loss: 0.0635 - accuracy: 0.9939 - val_loss: 0.2053 - val_accuracy: 0.9341\n",
      "Epoch 57/100\n",
      "172/172 [==============================] - 20s 118ms/step - loss: 0.0615 - accuracy: 0.9943 - val_loss: 0.2039 - val_accuracy: 0.9343\n",
      "Epoch 58/100\n",
      "172/172 [==============================] - 21s 120ms/step - loss: 0.0595 - accuracy: 0.9947 - val_loss: 0.2024 - val_accuracy: 0.9350\n",
      "Epoch 59/100\n",
      "172/172 [==============================] - 20s 118ms/step - loss: 0.0576 - accuracy: 0.9950 - val_loss: 0.2011 - val_accuracy: 0.9352\n",
      "Epoch 60/100\n",
      "172/172 [==============================] - 20s 118ms/step - loss: 0.0559 - accuracy: 0.9955 - val_loss: 0.1998 - val_accuracy: 0.9352\n",
      "Epoch 61/100\n",
      "172/172 [==============================] - 20s 119ms/step - loss: 0.0542 - accuracy: 0.9959 - val_loss: 0.1986 - val_accuracy: 0.9358\n",
      "Epoch 62/100\n",
      "172/172 [==============================] - 20s 118ms/step - loss: 0.0526 - accuracy: 0.9960 - val_loss: 0.1974 - val_accuracy: 0.9359\n",
      "Epoch 63/100\n",
      "172/172 [==============================] - 20s 118ms/step - loss: 0.0511 - accuracy: 0.9964 - val_loss: 0.1963 - val_accuracy: 0.9365\n",
      "Epoch 64/100\n",
      "172/172 [==============================] - 20s 118ms/step - loss: 0.0495 - accuracy: 0.9969 - val_loss: 0.1951 - val_accuracy: 0.9369\n",
      "Epoch 65/100\n",
      "172/172 [==============================] - 20s 118ms/step - loss: 0.0480 - accuracy: 0.9973 - val_loss: 0.1940 - val_accuracy: 0.9370\n",
      "Epoch 66/100\n",
      "172/172 [==============================] - 20s 119ms/step - loss: 0.0467 - accuracy: 0.9975 - val_loss: 0.1930 - val_accuracy: 0.9379\n",
      "Epoch 67/100\n",
      "172/172 [==============================] - 20s 118ms/step - loss: 0.0454 - accuracy: 0.9977 - val_loss: 0.1920 - val_accuracy: 0.9381\n",
      "Epoch 68/100\n",
      "172/172 [==============================] - 20s 118ms/step - loss: 0.0442 - accuracy: 0.9978 - val_loss: 0.1910 - val_accuracy: 0.9385\n",
      "Epoch 69/100\n",
      "172/172 [==============================] - 20s 118ms/step - loss: 0.0429 - accuracy: 0.9980 - val_loss: 0.1900 - val_accuracy: 0.9392\n",
      "Epoch 70/100\n",
      "172/172 [==============================] - 20s 118ms/step - loss: 0.0417 - accuracy: 0.9981 - val_loss: 0.1892 - val_accuracy: 0.9390\n",
      "Epoch 71/100\n",
      "172/172 [==============================] - 20s 118ms/step - loss: 0.0405 - accuracy: 0.9984 - val_loss: 0.1883 - val_accuracy: 0.9392\n",
      "Epoch 72/100\n",
      "172/172 [==============================] - 20s 119ms/step - loss: 0.0395 - accuracy: 0.9985 - val_loss: 0.1875 - val_accuracy: 0.9396\n",
      "Epoch 73/100\n",
      "172/172 [==============================] - 21s 120ms/step - loss: 0.0385 - accuracy: 0.9986 - val_loss: 0.1866 - val_accuracy: 0.9403\n",
      "Epoch 74/100\n",
      "172/172 [==============================] - 20s 119ms/step - loss: 0.0375 - accuracy: 0.9987 - val_loss: 0.1859 - val_accuracy: 0.9399\n",
      "Epoch 75/100\n",
      "172/172 [==============================] - 21s 124ms/step - loss: 0.0365 - accuracy: 0.9988 - val_loss: 0.1851 - val_accuracy: 0.9410\n",
      "Epoch 76/100\n",
      "172/172 [==============================] - 20s 118ms/step - loss: 0.0355 - accuracy: 0.9989 - val_loss: 0.1844 - val_accuracy: 0.9412\n",
      "Epoch 77/100\n",
      "172/172 [==============================] - 20s 118ms/step - loss: 0.0346 - accuracy: 0.9990 - val_loss: 0.1837 - val_accuracy: 0.9418\n",
      "Epoch 78/100\n",
      "172/172 [==============================] - 20s 118ms/step - loss: 0.0338 - accuracy: 0.9990 - val_loss: 0.1830 - val_accuracy: 0.9418\n",
      "Epoch 79/100\n",
      "172/172 [==============================] - 20s 117ms/step - loss: 0.0329 - accuracy: 0.9991 - val_loss: 0.1824 - val_accuracy: 0.9416\n",
      "Epoch 80/100\n",
      "172/172 [==============================] - 20s 118ms/step - loss: 0.0321 - accuracy: 0.9992 - val_loss: 0.1817 - val_accuracy: 0.9423\n",
      "Epoch 81/100\n",
      "172/172 [==============================] - 20s 117ms/step - loss: 0.0313 - accuracy: 0.9992 - val_loss: 0.1812 - val_accuracy: 0.9425\n",
      "Epoch 82/100\n",
      "172/172 [==============================] - 20s 117ms/step - loss: 0.0305 - accuracy: 0.9993 - val_loss: 0.1805 - val_accuracy: 0.9423\n",
      "Epoch 83/100\n",
      "172/172 [==============================] - 20s 117ms/step - loss: 0.0298 - accuracy: 0.9994 - val_loss: 0.1800 - val_accuracy: 0.9427\n",
      "Epoch 84/100\n",
      "172/172 [==============================] - 20s 117ms/step - loss: 0.0292 - accuracy: 0.9994 - val_loss: 0.1795 - val_accuracy: 0.9425\n",
      "Epoch 85/100\n",
      "172/172 [==============================] - 20s 119ms/step - loss: 0.0285 - accuracy: 0.9995 - val_loss: 0.1789 - val_accuracy: 0.9429\n",
      "Epoch 86/100\n",
      "172/172 [==============================] - 20s 117ms/step - loss: 0.0278 - accuracy: 0.9995 - val_loss: 0.1784 - val_accuracy: 0.9427\n",
      "Epoch 87/100\n",
      "172/172 [==============================] - 20s 117ms/step - loss: 0.0272 - accuracy: 0.9995 - val_loss: 0.1778 - val_accuracy: 0.9429\n",
      "Epoch 88/100\n",
      "172/172 [==============================] - 20s 117ms/step - loss: 0.0266 - accuracy: 0.9995 - val_loss: 0.1774 - val_accuracy: 0.9434\n",
      "Epoch 89/100\n",
      "172/172 [==============================] - 20s 117ms/step - loss: 0.0260 - accuracy: 0.9995 - val_loss: 0.1770 - val_accuracy: 0.9432\n",
      "Epoch 90/100\n",
      "172/172 [==============================] - 20s 117ms/step - loss: 0.0254 - accuracy: 0.9995 - val_loss: 0.1765 - val_accuracy: 0.9434\n",
      "Epoch 91/100\n",
      "172/172 [==============================] - 20s 118ms/step - loss: 0.0249 - accuracy: 0.9995 - val_loss: 0.1761 - val_accuracy: 0.9430\n",
      "Epoch 92/100\n",
      "172/172 [==============================] - 20s 119ms/step - loss: 0.0244 - accuracy: 0.9996 - val_loss: 0.1757 - val_accuracy: 0.9432\n",
      "Epoch 93/100\n",
      "172/172 [==============================] - 20s 117ms/step - loss: 0.0238 - accuracy: 0.9996 - val_loss: 0.1752 - val_accuracy: 0.9430\n",
      "Epoch 94/100\n",
      "172/172 [==============================] - 20s 117ms/step - loss: 0.0233 - accuracy: 0.9996 - val_loss: 0.1749 - val_accuracy: 0.9432\n",
      "Epoch 95/100\n",
      "172/172 [==============================] - 20s 117ms/step - loss: 0.0229 - accuracy: 0.9996 - val_loss: 0.1746 - val_accuracy: 0.9434\n",
      "Epoch 96/100\n",
      "172/172 [==============================] - 20s 117ms/step - loss: 0.0224 - accuracy: 0.9997 - val_loss: 0.1742 - val_accuracy: 0.9434\n",
      "Epoch 97/100\n",
      "172/172 [==============================] - 21s 121ms/step - loss: 0.0220 - accuracy: 0.9998 - val_loss: 0.1738 - val_accuracy: 0.9432\n",
      "Epoch 98/100\n",
      "172/172 [==============================] - 21s 124ms/step - loss: 0.0215 - accuracy: 0.9998 - val_loss: 0.1735 - val_accuracy: 0.9430\n",
      "Epoch 99/100\n",
      "172/172 [==============================] - 21s 125ms/step - loss: 0.0211 - accuracy: 0.9998 - val_loss: 0.1732 - val_accuracy: 0.9430\n",
      "Epoch 100/100\n",
      "172/172 [==============================] - 23s 132ms/step - loss: 0.0207 - accuracy: 0.9998 - val_loss: 0.1728 - val_accuracy: 0.9434\n"
     ]
    }
   ],
   "source": [
    "# ÁºñËØëÊ®°Âûã\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.00001, decay_steps=1000, decay_rate=0.90, staircase=True)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# ËÆ≠ÁªÉÁ¨¨‰∏ÄÈò∂ÊÆµ\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "history_stage1 = model.fit(train_dataset,\n",
    "                    validation_data=validation_dataset,\n",
    "                    epochs=100, \n",
    "                    callbacks=[early_stopping]\n",
    "                    )\n",
    "\n",
    "# ‰øùÂ≠òÁ¨¨‰∏ÄÈò∂ÊÆµÊ®°Âûã\n",
    "# model.save('./model/stage1_model.h5')\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------- Á¨¨‰∫åÈò∂ÊÆµËÆ≠ÁªÉ ---------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 54954 files belonging to 15 classes.\n",
      "Using 43964 files for training.\n",
      "Found 54954 files belonging to 15 classes.\n",
      "Using 10990 files for validation.\n"
     ]
    }
   ],
   "source": [
    "# üîπ Êï∞ÊçÆÈõÜË∑ØÂæÑ\n",
    "base_dir = './dataset'\n",
    "train_dir = os.path.join(base_dir, 'train2')\n",
    "valid_dir = os.path.join(base_dir, 'train2')\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# Âä†ËΩΩÊï∞ÊçÆÈõÜ\n",
    "train_dataset_raw = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    train_dir, validation_split=0.2, subset=\"training\", seed=12,\n",
    "    batch_size=BATCH_SIZE, image_size=IMG_SIZE)\n",
    "\n",
    "validation_dataset_raw = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    valid_dir, validation_split=0.2, subset=\"validation\", seed=12,\n",
    "    batch_size=BATCH_SIZE, image_size=IMG_SIZE)\n",
    "\n",
    "# Êï∞ÊçÆÈ¢ÑÂ§ÑÁêÜÔºàÂêåÁ¨¨‰∏ÄÈò∂ÊÆµÔºâ\n",
    "train_dataset = (train_dataset_raw\n",
    "                 .map(preprocess_image, num_parallel_calls=AUTOTUNE)\n",
    "                 .cache()\n",
    "                 .shuffle(1000)\n",
    "                 .prefetch(AUTOTUNE))\n",
    "\n",
    "validation_dataset = (validation_dataset_raw\n",
    "                      .map(preprocess_image, num_parallel_calls=AUTOTUNE)\n",
    "                      .cache()\n",
    "                      .prefetch(AUTOTUNE))\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "# Âä†ËΩΩÁ¨¨‰∏ÄÈò∂ÊÆµÊ®°Âûã\n",
    "# model = tf.keras.models.load_model('./model/stage1_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "687/687 [==============================] - 53s 69ms/step - loss: 0.0027 - accuracy: 0.9996 - val_loss: 0.1025 - val_accuracy: 0.9681\n",
      "Epoch 2/100\n",
      "687/687 [==============================] - 47s 69ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0803 - val_accuracy: 0.9751\n",
      "Epoch 3/100\n",
      "687/687 [==============================] - 47s 69ms/step - loss: 4.1949e-04 - accuracy: 1.0000 - val_loss: 0.0621 - val_accuracy: 0.9823\n",
      "Epoch 4/100\n",
      "687/687 [==============================] - 47s 69ms/step - loss: 2.8958e-04 - accuracy: 1.0000 - val_loss: 0.0621 - val_accuracy: 0.9820\n",
      "Epoch 5/100\n",
      "687/687 [==============================] - 47s 69ms/step - loss: 2.1270e-04 - accuracy: 1.0000 - val_loss: 0.0620 - val_accuracy: 0.9828\n",
      "Epoch 6/100\n",
      "687/687 [==============================] - 47s 69ms/step - loss: 1.5686e-04 - accuracy: 1.0000 - val_loss: 0.0638 - val_accuracy: 0.9824\n",
      "Epoch 7/100\n",
      "687/687 [==============================] - 47s 69ms/step - loss: 1.1561e-04 - accuracy: 1.0000 - val_loss: 0.0640 - val_accuracy: 0.9823\n",
      "Epoch 8/100\n",
      "687/687 [==============================] - 47s 69ms/step - loss: 8.4235e-05 - accuracy: 1.0000 - val_loss: 0.0647 - val_accuracy: 0.9823\n",
      "Epoch 9/100\n",
      "687/687 [==============================] - 47s 69ms/step - loss: 6.0385e-05 - accuracy: 1.0000 - val_loss: 0.0661 - val_accuracy: 0.9820\n",
      "Epoch 10/100\n",
      "687/687 [==============================] - 47s 69ms/step - loss: 4.2859e-05 - accuracy: 1.0000 - val_loss: 0.0655 - val_accuracy: 0.9826\n",
      "Epoch 11/100\n",
      "687/687 [==============================] - 48s 69ms/step - loss: 3.0373e-05 - accuracy: 1.0000 - val_loss: 0.0683 - val_accuracy: 0.9824\n",
      "Epoch 12/100\n",
      "687/687 [==============================] - 47s 69ms/step - loss: 2.1431e-05 - accuracy: 1.0000 - val_loss: 0.0681 - val_accuracy: 0.9831\n",
      "Epoch 13/100\n",
      "687/687 [==============================] - 47s 69ms/step - loss: 1.4986e-05 - accuracy: 1.0000 - val_loss: 0.0716 - val_accuracy: 0.9830\n",
      "Epoch 14/100\n",
      "687/687 [==============================] - 47s 69ms/step - loss: 1.0487e-05 - accuracy: 1.0000 - val_loss: 0.0735 - val_accuracy: 0.9826\n",
      "Epoch 15/100\n",
      "687/687 [==============================] - 48s 70ms/step - loss: 7.3347e-06 - accuracy: 1.0000 - val_loss: 0.0743 - val_accuracy: 0.9827\n"
     ]
    }
   ],
   "source": [
    "# ÁºñËØëÊ®°Âûã\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.00001, decay_steps=1000, decay_rate=0.99, staircase=True)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# ËÆ≠ÁªÉÁ¨¨‰∏ÄÈò∂ÊÆµ\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "history_stage2 = model.fit(train_dataset, validation_data=validation_dataset,\n",
    "                    epochs=100, callbacks=[early_stopping])\n",
    "# ‰øùÂ≠òÁ¨¨‰∏ÄÈò∂ÊÆµÊ®°Âûã\n",
    "model.save('./model/stage2_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------- Á¨¨‰∏âÈò∂ÊÆµËÆ≠ÁªÉ ---------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîπ Ââ™ÊûùÂèÇÊï∞ÈÖçÁΩÆ\n",
    "PRUNING_PARAMS = {\n",
    "    'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(\n",
    "        initial_sparsity=0.30,\n",
    "        final_sparsity=0.60,\n",
    "        begin_step=0,\n",
    "        end_step=2000,\n",
    "        frequency=100\n",
    "    )\n",
    "}\n",
    "\n",
    "# Âä†ËΩΩÁ¨¨‰∫åÈò∂ÊÆµÊ®°Âûã\n",
    "# model = tf.keras.models.load_model('./model/stage2_model.h5')\n",
    "\n",
    "# üîπ ÂàÜÁ¶ª Rescaling Â±ÇÂíåÂü∫Á°ÄÊ®°Âûã\n",
    "rescale_layer = model.layers[0]  # ÊèêÂèñ Rescaling Â±Ç    \n",
    "prunable_model = tf.keras.Sequential(model.layers[1:])  # ÊéíÈô§ Rescaling ÂêéÁöÑÊ®°Âûã\n",
    "\n",
    "# üîπ Â∫îÁî®Ââ™ÊûùÂà∞ÂèØÂâ™ÊûùÈÉ®ÂàÜ\n",
    "with tfmot.sparsity.keras.prune_scope():  # Á°Æ‰øùÂâ™Êûù‰ΩúÁî®ÂüüÊ≠£Á°Æ\n",
    "    pruned_model = tfmot.sparsity.keras.prune_low_magnitude(\n",
    "        prunable_model, **PRUNING_PARAMS\n",
    "    )\n",
    "\n",
    "# üîπ ÈáçÊñ∞ÁªÑÂêàÊ®°Âûã\n",
    "final_model = tf.keras.Sequential([\n",
    "    rescale_layer,  # ÂâçÁΩÆ Rescaling\n",
    "    pruned_model    # Ââ™ÊûùÂêéÁöÑÊ®°ÂûãÈÉ®ÂàÜ\n",
    "])\n",
    "\n",
    "# ÁºñËØëÊ®°Âûã\n",
    "final_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîπ Êï∞ÊçÆÂ¢ûÂº∫\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "    tf.keras.layers.RandomRotation(factor=(-0.125,0.125),fill_mode=\"nearest\"),\n",
    "    tf.keras.layers.RandomZoom(0.25,fill_mode=\"nearest\"),\n",
    "    tf.keras.layers.RandomTranslation(height_factor=0.25, width_factor=0.25),\n",
    "    tf.keras.layers.RandomBrightness(0.25),\n",
    "    tf.keras.layers.RandomContrast(0.3)\n",
    "])\n",
    "\n",
    "# È¢ÑÂ§ÑÁêÜÂáΩÊï∞ÔºàÊ∑ªÂä†Â¢ûÂº∫Ôºâ\n",
    "def preprocess_image_aug(image, label):\n",
    "    image = data_augmentation(image)\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 54954 files belonging to 15 classes.\n",
      "Using 43964 files for training.\n",
      "Found 54954 files belonging to 15 classes.\n",
      "Using 10990 files for validation.\n"
     ]
    }
   ],
   "source": [
    "# Âä†ËΩΩÊï∞ÊçÆÈõÜÔºà‰ΩøÁî®Êñ∞Êï∞ÊçÆÈõÜÔºâ\n",
    "base_dir = './dataset'\n",
    "train_dir = os.path.join(base_dir, 'train2')\n",
    "valid_dir = os.path.join(base_dir, 'train2')\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "train_dataset_raw = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    train_dir, validation_split=0.2, subset=\"training\", seed=12,\n",
    "    batch_size=BATCH_SIZE, image_size=IMG_SIZE)\n",
    "\n",
    "validation_dataset_raw = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    valid_dir, validation_split=0.2, subset=\"validation\", seed=12,\n",
    "    batch_size=BATCH_SIZE, image_size=IMG_SIZE)\n",
    "\n",
    "# Êï∞ÊçÆÈ¢ÑÂ§ÑÁêÜÔºàÂ∫îÁî®Â¢ûÂº∫Ôºâ\n",
    "train_dataset = (train_dataset_raw\n",
    "                 .map(preprocess_image_aug, num_parallel_calls=AUTOTUNE)\n",
    "                 .cache()\n",
    "                 .shuffle(1000)\n",
    "                 .prefetch(AUTOTUNE))\n",
    "\n",
    "validation_dataset = (validation_dataset_raw\n",
    "                      .map(preprocess_image, num_parallel_calls=AUTOTUNE)\n",
    "                      .cache()\n",
    "                      .prefetch(AUTOTUNE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîπ Ê∑ªÂä†Ââ™ÊûùÂõûË∞É\n",
    "pruning_callbacks = [\n",
    "    tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "    tfmot.sparsity.keras.PruningSummaries(log_dir='./logs_pruning')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mÂú®ÂΩìÂâçÂçïÂÖÉÊ†ºÊàñ‰∏ä‰∏Ä‰∏™ÂçïÂÖÉÊ†º‰∏≠ÊâßË°å‰ª£Á†ÅÊó∂ Kernel Â¥©Ê∫É„ÄÇ\n",
      "\u001b[1;31mËØ∑Êü•ÁúãÂçïÂÖÉÊ†º‰∏≠ÁöÑ‰ª£Á†ÅÔºå‰ª•Á°ÆÂÆöÊïÖÈöúÁöÑÂèØËÉΩÂéüÂõ†„ÄÇ\n",
      "\u001b[1;31mÂçïÂáª<a href='https://aka.ms/vscodeJupyterKernelCrash'>Ê≠§Â§Ñ</a>‰∫ÜËß£ËØ¶ÁªÜ‰ø°ÊÅØ„ÄÇ\n",
      "\u001b[1;31mÊúâÂÖ≥Êõ¥Â§öËØ¶ÁªÜ‰ø°ÊÅØÔºåËØ∑Êü•Áúã Jupyter <a href='command:jupyter.viewOutput'>log</a>„ÄÇ"
     ]
    }
   ],
   "source": [
    "# ËÆ≠ÁªÉÁ¨¨‰∏âÈò∂ÊÆµ\n",
    "history_stage3 = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=validation_dataset,\n",
    "    epochs=10,\n",
    "    callbacks=[early_stopping, pruning_callbacks]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÂéªÈô§Ââ™ÊûùÂåÖË£Ö\n",
    "final_model = tfmot.sparsity.keras.strip_pruning(model)\n",
    "final_model.save('./model/stage3_pruned_final.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Âä†ËΩΩÂâ™ÊûùÂêéÁöÑÊ®°Âûã\n",
    "model = tf.keras.models.load_model('./model/stage3_pruned_final.h5')\n",
    "\n",
    "def representative_dataset():\n",
    "    for image_batch, _ in tqdm(validation_dataset_raw.take(500), desc=\"Processing\"):\n",
    "        yield [tf.cast(image_batch, tf.float32)]\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "tflite_model_quant = converter.convert()\n",
    "\n",
    "# ---- 4. Âä®ÊÄÅÁîüÊàêÂ∏¶Êó∂Èó¥ÁöÑÊñá‰ª∂Âêç ----\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "output_tflite_path = f'./model/model_{timestamp}.tflite'  # Êñ∞Êñá‰ª∂ÂêçÊ†ºÂºè\n",
    "\n",
    "with open(output_tflite_path, 'wb') as f:\n",
    "    f.write(tflite_model_quant)\n",
    "\n",
    "target_dir = \"model\"\n",
    "# Áõ¥Êé•ÂåπÈÖçÂΩìÂâçÁõÆÂΩï‰∏ãÁöÑ .h5 Êñá‰ª∂\n",
    "for file in os.listdir(target_dir):\n",
    "    if file.endswith(\".h5\"):\n",
    "        file_path = os.path.join(target_dir, file)\n",
    "        try:\n",
    "            os.remove(file_path)\n",
    "            print(f\"Â∑≤Âà†Èô§: {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Âà†Èô§Â§±Ë¥• [{file_path}]: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ê∑∑Ê∑ÜÁü©Èòµ\n",
    "y_pred = np.argmax(model.predict(validation_dataset), axis=1)\n",
    "y_true = np.concatenate([labels.numpy() for _, labels in validation_dataset])\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, cmap=\"Blues\", fmt=\"d\", \n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from librariy import polt_improved\n",
    "\n",
    "stage_names = [\"Stage 1\", \"Stage 2\", \"Stage 3 (Pruning)\"]\n",
    "history_list = [history_stage1, history_stage2, history_stage3]\n",
    "polt_improved.plot_combined_curves_improved(history_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
